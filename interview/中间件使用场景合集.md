# 中间件使用场景合集

## MySQL

## Redis

### 分布式锁

- 定时任务控制，后台微服务多节点部署，执行定时任务时需要抢占分布式锁，避免多节点重复执行定时任务

- 本地缓存刷新时，某些配置项需要频繁读数据库，并具有一定计算量，通过分布式锁控制单线程去获取，避免对数据库造成压力，并且降低cpu开销。其他线程则自旋一段时间（自旋失败抛出异常）

### 分布式缓存

- 加速访问，将部分数据缓存到redis。缓存数据可能包含数据库查询返回的直接结果，或经过计算处理结果
  - 响应速度：Mysql在`ms`级，redis在`μs`级
- 缓存高频数据，降低数据库压力

## Kafka

### 通过注解配置消费者

```java
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Component;

@Component
public class KafkaConsumer {

    @KafkaListener(
      	topics = "my-topic", 
      	groupId = "my-group", 
      	properties = {
        "auto.offset.reset=earliest",
        "max.poll.interval.ms=5000"
        }
    )
    public void listen(String message) {
        System.out.println("Received Message: " + message);
    }
}
```

#### 位移提交

通过`spring.kafka.listener.ack-mode`配置 或 通过`org.springframework.kafka.listener.config.ContainerProperties#ackMode`配置

|          模式          |                       行为                       |          适用场景          |
| :--------------------: | :----------------------------------------------: | :------------------------: |
|      **`RECORD`**      |          每条消息处理成功后立即提交位移          |     高可靠性，但性能低     |
|  **`BATCH`**（默认）   |     批量处理完成后提交位移（攒一批提交一次）     |     平衡可靠性和吞吐量     |
|       **`TIME`**       |      按时间间隔提交位移（需配置 `ackTime`）      |   允许少量重复，提高吞吐   |
|      **`COUNT`**       |  按处理的消息数量提交位移（需配置 `ackCount`）   | 类似 `BATCH`，但按条数控制 |
|      **`MANUAL`**      | 用户手动调用 `Acknowledgment.acknowledge()` 提交 |      完全控制提交时机      |
| **`MANUAL_IMMEDIATE`** |           手动调用后立即提交（非批量）           |     需低延迟手动提交时     |

```java
@KafkaListener(topics = "my-topic", groupId = "my-group")
public void listen(String message, Acknowledgment ack) {
    try {
        System.out.println("Processing: " + message);
        // 业务逻辑...
        ack.acknowledge();  // 手动提交位移
    } catch (Exception e) {
        // 处理异常（不提交位移，会触发重试）
    }
}
```

### 订阅新闻主题拉取新闻

```properties
bootstrap.servers = "***"
topics = "***"
group.id = "***"
enable.auto.commit = true
security.protocol = "SASL_PLAINTEXT"
sasl.mechanism = "SCRM-SHA-512"
sasl.jaas.config = "username password"
```

### 安全日志上报

```properties
bootstrap.servers = "***"
acks = 1 # 仅主副本确认即可
retries = 1 # 不做过多重试，影响性能
batch.size = 32768 # 32KB
compress.type = gzip # CPU开销大、压缩率高，对带宽影响小，但增加了延时
linger.ms = 5 # 等待5ms
```

## Zookeeper

### [elastic job](#elastic job)分布式定时任务管理

### Dubbo服务注册

## Apollo

### 配置管理

### 热更新

## 微服务网关

Http > Dubbo转换

- 统一入口，隔离区 > 核心区
- 请求路由到微服务
- 流量控制、负载均衡
- 协议转换Http > Dubbo
- 请求/响应改写
- 认证与鉴权

## Nginx

- 统一入口，实现负载均衡
- 路由不同应用的微服务
- 反向代理，隐藏后端服务
- 静态资源托管 -> apisix改造后前端改用MinIO部署（一个高性能的分布式对象存储系统）

## S3协议存储网关

## 其他

### 本地缓存 Caffeine

- 缓存更新频率低，能容忍延迟的数据，如配置项或人员部门等信息
- 加速访问，本地内存读速度极快

```java
Cache<String, Object> cache = new Caffeine.newBuilder()
  	.maximunSize(1000) // 最多1000个元素
  	.expireAfterWrite(12, TimeUnit.HOURS) // 写入的本地缓存12小时后过期
  	.build();

// 自动刷新（校验时机，get()时）
LoadingCache<String, String> cache = Caffeine.newBuilder()
    .refreshAfterWrite(5, TimeUnit.SECONDS)  // 5秒后异步刷新
    .expireAfterWrite(30, TimeUnit.SECONDS)  // 30秒后强制过期
    .build(key -> fetchDataFromDB(key)); // 设置缓存刷新方法
```

### elastic job

当当网开源的分布式调度解决方案，基于 Quartz 二次开发，主要用于解决分布式环境下的定时任务扩展性、高可用及弹性调度问题

- 分片机制（需手动在任务中根据分片号、分片参数实现）
  - 任务可拆分为多个分片，由不同节点处理
- 弹性调度
  - 节点增减时，自动触发分片重分配
  - 节点宕机后，未完成的分片由其他节点接管
- 分布式协调
  - 通过 Zookeeper 选举主节点，负责分片分配和状态同步
  - 利用 Zookeeper 的 Watcher 机制监听节点变化，实时调整任务分配



```java
// 可分片的任务示例
@Component("testJob")
public class TestJob implements SimpleJob {
    @Override
    public void execute(ShardingContext shardingContext) {
        int shardingItem = shardingContext.getShardingItem();
        String shardingParameter = shardingContext.getShardingParameter();
        System.out.printf("current thread is %s, shardingItem is %d, shardingParameter %s\n",
                Thread.currentThread().getName(),
                shardingItem,
                shardingParameter);
    }
}

// 配置定时任务
@Configuration
public class SimpleJobConfig {

    @Resource
    private ZookeeperRegistryCenter zookeeperRegistryCenter;

    @Bean(initMethod = "schedule")
    public ScheduleJobBootstrap scheduleJobBootstrap(@Qualifier("testJob") SimpleJob simpleJob){
        return new ScheduleJobBootstrap(
                zookeeperRegistryCenter,
                simpleJob,
                JobConfiguration.newBuilder("jonName", 3)
                        .shardingItemParameters("0=A,1=B,2=C")
                        .cron("0/10 * * * * ?")
                        .overwrite(true)
                        .build()
        );
    }
}
```

#### 核心代码

- zookeeper

  - ```java
    // 连接zookeeper ip端口，并指定namespace
    ZookeeperConfiguration zookeeperConfiguration = new ZookeeperConfiguration("127.0.0.1:2181", "test");
    ```

  - ```java
    //连接zookeeper时创建临时顺序节点，抢占主节点身份（以任务为维度）
    this.client.create().creatingParentContainersIfNeeded()
      	.withProtection()
      	.withMode(CreateMode.EPHEMERAL_SEQUENTIAL))// 临时顺序节点
      	.inBackground(callback))
      	.forPath(ZKPaths.makePath(this.latchPath, "latch-"), LeaderSelector.getIdBytes(this.id));
    ```

  - ```java
    // 注册各种监听器，如主节点宕机等
    this.listenerManager.startAllListeners();
    ```

  - ```java
    // 注册当前服务器（应用程序实例）的元数据到zk，表示可以参与任务调度管理       
    this.jobNodeStorage.fillJobNode(
      this.serverNode.getServerNode(JobRegistry.getInstance().getJobInstance(this.jobName).getServerIp()),
      enabled ? ServerStatus.ENABLED.name() : ServerStatus.DISABLED.name()
    );
    
    // 注册当前任务实例到zk的临时节点，表示已上线并可以参与任务分配
    this.jobNodeStorage.fillEphemeralJobNode(
      	this.instanceNode.getLocalInstancePath(), // 当前任务实例在zk的路径
      	this.instanceNode.getLocalInstanceValue()
    );
    ```

  - 

